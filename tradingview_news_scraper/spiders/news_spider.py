# requirements.txt
# scrapy
# selenium
# webdriver-manager

import scrapy
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import os
import requests
from urllib.parse import urljoin, urlparse
from datetime import datetime

class TradingViewNewsSpider(scrapy.Spider):
    name = 'tradingview_news'
    start_urls = ['https://www.tradingview.com/news-flow/?symbol=BINANCE:BTCUSDT']
    
    def __init__(self):
        # è¨­ç½®Chromeé¸é …
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼ï¼Œå¦‚æœéœ€è¦çœ‹åˆ°ç€è¦½å™¨é‹è¡Œè«‹è¨»é‡‹æ‰
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # åˆå§‹åŒ–WebDriver
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
        # XPATHè·¯å¾‘é…ç½® - è«‹åœ¨æ­¤è™•å¡«å…¥æ‚¨çš„XPATH
        self.xpaths = {
            'news_list': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div',  # æ–°èåˆ—è¡¨å®¹å™¨çš„XPATH
            'news_items': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div/a',  # æ–°èé …ç›®çš„XPATHï¼ˆå¯é»æ“Šçš„é€£çµï¼‰
            'right_panel': '//*[@id="news-screener-page"]/div/div/div/div[3]',  # å³å´é¢æ¿çš„XPATH
            'panel_title': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/h2',  # å³å´é¢æ¿æ¨™é¡Œçš„XPATH
            'panel_content': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/div[3]/div/div[1]/div[2]/span',  # å³å´é¢æ¿å…§å®¹çš„XPATH
            'panel_symbols': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/div[3]/div/div[1]/div[1]',  # å³å´é¢æ¿ç¬¦è™Ÿçš„XPATH
            'close_button': '',  # é—œé–‰æŒ‰éˆ•çš„XPATHï¼ˆå¯é¸ï¼‰
            'list_title': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div/a[1]/article/div/div'  # åˆ—è¡¨ä¸­æ¨™é¡Œçš„XPATHï¼ˆç›¸å°æ–¼news_itemsï¼‰
        }
        
        # åˆå§‹åŒ–Markdownè¼¸å‡º
        self.markdown_content = []
        self.output_filename = f'tradingview_news_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        
        # å‰µå»ºåœ–ç‰‡ä¿å­˜ç›®éŒ„
        self.images_dir = f'images_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        os.makedirs(self.images_dir, exist_ok=True)
        
        # åœ–ç‰‡ä¸‹è¼‰è¨­ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def parse(self, response):
        """ä¸»è¦è§£æå‡½æ•¸"""
        self.driver.get(response.url)
        
        # åˆå§‹åŒ–Markdownæ–‡ä»¶æ¨™é¡Œ
        self.markdown_content.append(f"# TradingView BTCUSDT æ–°èå ±å‘Š\n")
        self.markdown_content.append(f"**çˆ¬å–æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.markdown_content.append(f"**ä¾†æº**: {response.url}\n\n")
        self.markdown_content.append("---\n\n")
        
        try:
            # ç­‰å¾…æ–°èåˆ—è¡¨åŠ è¼‰
            if self.xpaths['news_list']:
                self.wait.until(EC.presence_of_element_located((By.XPATH, self.xpaths['news_list'])))
                self.logger.info("æ–°èåˆ—è¡¨åŠ è¼‰å®Œæˆ")
            
            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ›´å¤šæ–°è
            self.scroll_and_load_news()
            
            # ç²å–æ‰€æœ‰æ–°èé …ç›®
            if not self.xpaths['news_items']:
                self.logger.error("è«‹è¨­ç½® news_items çš„XPATHè·¯å¾‘")
                return
                
            news_items = self.driver.find_elements(By.XPATH, self.xpaths['news_items'])
            self.logger.info(f"æ‰¾åˆ° {len(news_items)} æ¢æ–°è")
            
            # éæ­·æ¯å€‹æ–°èé …ç›®
            for index, news_item in enumerate(news_items):
                try:
                    # ç²å–æ–°èæ¨™é¡Œ
                    title = self.get_news_title(news_item)
                    
                    # é»æ“Šæ–°èé …ç›®
                    self.driver.execute_script("arguments[0].click();", news_item)
                    
                    # ç­‰å¾…å³å´é¢æ¿åŠ è¼‰
                    time.sleep(2)
                    
                    # æå–æ–°èè©³ç´°å…§å®¹
                    news_data = self.extract_news_details(title, index)
                    
                    if news_data:
                        # æ·»åŠ åˆ°Markdownå…§å®¹
                        self.add_to_markdown(news_data)
                        yield news_data
                    
                    # é—œé–‰å³å´é¢æ¿
                    self.close_right_panel()
                    
                except Exception as e:
                    self.logger.error(f"è™•ç†ç¬¬ {index+1} æ¢æ–°èæ™‚å‡ºéŒ¯: {str(e)}")
                    continue
                    
        except TimeoutException:
            self.logger.error("é é¢åŠ è¼‰è¶…æ™‚")
        except Exception as e:
            self.logger.error(f"çˆ¬å–éç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        self.save_markdown_file()
    
    def scroll_and_load_news(self):
        """æ»¾å‹•é é¢åŠ è¼‰æ›´å¤šæ–°è"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # æ»¾å‹•åˆ°é é¢åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # ç­‰å¾…åŠ è¼‰
            
            # è¨ˆç®—æ–°çš„é é¢é«˜åº¦
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
                
            last_height = new_height
        
        self.logger.info("å®Œæˆæ»¾å‹•åŠ è¼‰")
    
    def get_news_title(self, news_item):
        """ç²å–æ–°èæ¨™é¡Œ"""
        try:
            if self.xpaths['list_title']:
                # ä½¿ç”¨ç›¸å°XPATHæŸ¥æ‰¾æ¨™é¡Œ
                title_element = news_item.find_element(By.XPATH, self.xpaths['list_title'])
                title = title_element.text.strip()
                if title:
                    return title
            
            # å¦‚æœæ²’æœ‰è¨­ç½®XPATHæˆ–æ‰¾ä¸åˆ°ï¼Œè¿”å›å…ƒç´ æ–‡æœ¬
            return news_item.text.strip() or "ç„¡æ¨™é¡Œ"
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨™é¡Œæ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ¨™é¡Œ"
    
    def extract_news_details(self, title, index):
        """æå–æ–°èè©³ç´°å…§å®¹"""
        try:
            # ç­‰å¾…å³å´é¢æ¿å‡ºç¾
            if not self.xpaths['right_panel']:
                self.logger.error("è«‹è¨­ç½® right_panel çš„XPATHè·¯å¾‘")
                return None
                
            right_panel = self.wait.until(
                EC.presence_of_element_located((By.XPATH, self.xpaths['right_panel']))
            )
            
            # æå–æ¨™é¡Œï¼ˆå¾å³å´é¢æ¿ï¼‰
            panel_title = self.extract_panel_title()
            
            # æå–å…§å®¹å’Œåœ–ç‰‡
            content_data = self.extract_content()
            
            # æå–ç›¸é—œç¬¦è™Ÿ
            symbols = self.extract_symbols()
            
            news_data = {
                'index': index + 1,
                'list_title': title,  # åˆ—è¡¨ä¸­çš„æ¨™é¡Œ
                'panel_title': panel_title,  # å³å´é¢æ¿ä¸­çš„æ¨™é¡Œ
                'content': content_data['text'],  # æ–‡æœ¬å…§å®¹
                'images': content_data['images'],  # åœ–ç‰‡ä¿¡æ¯
                'symbols': symbols,
                'url': self.driver.current_url
            }
            
            self.logger.info(f"æˆåŠŸæå–ç¬¬ {index+1} æ¢æ–°è: {panel_title[:50]}... (åŒ…å« {len(content_data['images'])} å¼µåœ–ç‰‡)")
            return news_data
            
        except TimeoutException:
            self.logger.error(f"ç¬¬ {index+1} æ¢æ–°èå³å´é¢æ¿åŠ è¼‰è¶…æ™‚")
            return None
        except Exception as e:
            self.logger.error(f"æå–ç¬¬ {index+1} æ¢æ–°èè©³æƒ…æ™‚å‡ºéŒ¯: {str(e)}")
            return None
    
    def extract_panel_title(self):
        """å¾å³å´é¢æ¿æå–æ¨™é¡Œ"""
        try:
            if self.xpaths['panel_title']:
                title_element = self.driver.find_element(By.XPATH, self.xpaths['panel_title'])
                return title_element.text.strip()
            else:
                self.logger.warning("æœªè¨­ç½® panel_title çš„XPATHè·¯å¾‘")
                return "ç„¡æ¨™é¡Œ"
        except NoSuchElementException:
            self.logger.warning("ä½¿ç”¨XPATHæœªæ‰¾åˆ°æ¨™é¡Œå…ƒç´ ")
            return "ç„¡æ¨™é¡Œ"
        except Exception as e:
            self.logger.error(f"æå–é¢æ¿æ¨™é¡Œæ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ¨™é¡Œ"
    
    def extract_content(self):
        """æå–æ–°èå…§å®¹ä¸¦ä¸‹è¼‰åœ–ç‰‡"""
        try:
            if self.xpaths['panel_content']:
                # æ”¯æŒå–®å€‹æˆ–å¤šå€‹å…§å®¹å…ƒç´ 
                content_elements = self.driver.find_elements(By.XPATH, self.xpaths['panel_content'])
                
                contents = []
                images = []
                
                for element in content_elements:
                    # æå–æ–‡æœ¬å…§å®¹
                    text = element.text.strip()
                    if text and len(text) > 10:  # éæ¿¾æ‰å¤ªçŸ­çš„æ–‡æœ¬
                        contents.append(text)
                    
                    # æŸ¥æ‰¾ä¸¦ä¸‹è¼‰åœ–ç‰‡
                    img_elements = element.find_elements(By.TAG_NAME, 'img')
                    for img in img_elements:
                        image_info = self.download_image(img)
                        if image_info:
                            images.append(image_info)
                
                # å»é‡ä¸¦åˆä½µå…§å®¹
                unique_contents = list(dict.fromkeys(contents))  # ä¿æŒé †åºçš„å»é‡
                text_content = ' '.join(unique_contents) if unique_contents else "ç„¡å…§å®¹"
                
                # è¿”å›æ–‡æœ¬å…§å®¹å’Œåœ–ç‰‡ä¿¡æ¯
                return {
                    'text': text_content,
                    'images': images
                }
            else:
                self.logger.warning("æœªè¨­ç½® panel_content çš„XPATHè·¯å¾‘")
                return {
                    'text': "ç„¡å…§å®¹",
                    'images': []
                }
                
        except Exception as e:
            self.logger.error(f"æå–å…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
            return {
                'text': "ç„¡å…§å®¹",
                'images': []
            }
    
    def extract_symbols(self):
        """æå–ç›¸é—œç¬¦è™Ÿ"""
        try:
            if self.xpaths['panel_symbols']:
                symbol_elements = self.driver.find_elements(By.XPATH, self.xpaths['panel_symbols'])
                
                symbols = []
                for element in symbol_elements:
                    text = element.text.strip()
                    if text:
                        symbols.append(text)
                
                return list(set(symbols)) if symbols else []
            else:
                self.logger.warning("æœªè¨­ç½® panel_symbols çš„XPATHè·¯å¾‘")
                return []
                
        except Exception as e:
            self.logger.error(f"æå–ç¬¦è™Ÿæ™‚å‡ºéŒ¯: {str(e)}")
            return []
    
    def close_right_panel(self):
        """é—œé–‰å³å´é¢æ¿"""
        try:
            if self.xpaths['close_button']:
                # ä½¿ç”¨è¨­ç½®çš„é—œé–‰æŒ‰éˆ•XPATH
                close_btn = self.driver.find_element(By.XPATH, self.xpaths['close_button'])
                close_btn.click()
                time.sleep(1)
                return
            
            # å¦‚æœæ²’æœ‰è¨­ç½®é—œé–‰æŒ‰éˆ•XPATHï¼Œé»æ“Šé¢æ¿å¤–éƒ¨å€åŸŸ
            self.driver.execute_script("document.body.click();")
            time.sleep(1)
            
        except NoSuchElementException:
            # å¦‚æœæ‰¾ä¸åˆ°é—œé–‰æŒ‰éˆ•ï¼Œå˜—è©¦é»æ“Šé é¢å…¶ä»–å€åŸŸ
            self.driver.execute_script("document.body.click();")
            time.sleep(1)
        except Exception as e:
            self.logger.error(f"é—œé–‰å³å´é¢æ¿æ™‚å‡ºéŒ¯: {str(e)}")
    
    def download_image(self, img_element):
        """ä¸‹è¼‰åœ–ç‰‡ä¸¦è¿”å›åœ–ç‰‡ä¿¡æ¯"""
        try:
            # ç²å–åœ–ç‰‡URL
            img_src = img_element.get_attribute('src')
            if not img_src:
                return None
            
            # è™•ç†ç›¸å°URL
            if img_src.startswith('//'):
                img_src = 'https:' + img_src
            elif img_src.startswith('/'):
                img_src = urljoin('https://www.tradingview.com', img_src)
            
            # ç²å–åœ–ç‰‡altæ–‡æœ¬ä½œç‚ºæè¿°
            img_alt = img_element.get_attribute('alt') or 'image'
            
            # ç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(img_src)
            file_extension = os.path.splitext(parsed_url.path)[1] or '.jpg'
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ¯«ç§’ç´šæ™‚é–“æˆ³
            filename = f"image_{timestamp}{file_extension}"
            filepath = os.path.join(self.images_dir, filename)
            
            # ä¸‹è¼‰åœ–ç‰‡
            response = self.session.get(img_src, timeout=10)
            response.raise_for_status()
            
            # ä¿å­˜åœ–ç‰‡
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.logger.info(f"åœ–ç‰‡å·²ä¸‹è¼‰: {filename}")
            
            return {
                'filename': filename,
                'filepath': filepath,
                'url': img_src,
                'alt': img_alt,
                'size': len(response.content)
            }
            
        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰åœ–ç‰‡æ™‚å‡ºéŒ¯: {str(e)}")
            return None
    
    def add_to_markdown(self, news_data):
        """å°‡æ–°èæ•¸æ“šæ·»åŠ åˆ°Markdownå…§å®¹ä¸­"""
        try:
            self.markdown_content.append(f"## {news_data['index']}. {news_data['panel_title']}\n\n")
            
            # å¦‚æœåˆ—è¡¨æ¨™é¡Œèˆ‡é¢æ¿æ¨™é¡Œä¸åŒï¼Œé¡¯ç¤ºåˆ—è¡¨æ¨™é¡Œ
            if news_data['list_title'] != news_data['panel_title'] and news_data['list_title'] != "ç„¡æ¨™é¡Œ":
                self.markdown_content.append(f"**Provider**: {news_data['list_title']}\n\n")
            
            # æ–°èå…§å®¹
            if news_data['content'] and news_data['content'] != "ç„¡å…§å®¹":
                self.markdown_content.append(f"**å…§å®¹**:\n{news_data['content']}\n\n")
            
            # åœ–ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if news_data.get('images') and len(news_data['images']) > 0:
                self.markdown_content.append(f"**åœ–ç‰‡** ({len(news_data['images'])} å¼µ):\n\n")
                for i, img in enumerate(news_data['images'], 1):
                    # æ·»åŠ åœ–ç‰‡åˆ°Markdown
                    self.markdown_content.append(f"{i}. ![{img['alt']}]({img['filepath']})\n")
                    if img['alt'] and img['alt'] != 'image':
                        self.markdown_content.append(f"   - æè¿°: {img['alt']}\n")
                    self.markdown_content.append(f"   - æ–‡ä»¶: `{img['filename']}`\n")
            
            # ç›¸é—œç¬¦è™Ÿ
            if news_data['symbols']:
                symbols_text = ", ".join([f"`{symbol}`" for symbol in news_data['symbols']])
                self.markdown_content.append(f"**ç›¸é—œç¬¦è™Ÿ**: {symbols_text}\n\n")
            
            # ä¾†æºé€£çµ
            if news_data['url']:
                self.markdown_content.append(f"**ä¾†æº**: [TradingView]({news_data['url']})\n\n")
            
            self.markdown_content.append("---\n\n")
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ Markdownå…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
    
    def save_markdown_file(self):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        try:
            # è¨ˆç®—ç¸½åœ–ç‰‡æ•¸é‡
            total_images = sum([len(line.split('![')) - 1 for line in self.markdown_content if '![' in line])
            
            # æ·»åŠ æ–‡ä»¶çµå°¾
            self.markdown_content.append(f"\n---\n")
            self.markdown_content.append(f"**å ±å‘Šç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            self.markdown_content.append(f"**ç¸½å…±çˆ¬å–**: {len([line for line in self.markdown_content if line.startswith('## ')])} æ¢æ–°è\n")
            self.markdown_content.append(f"**ç¸½å…±ä¸‹è¼‰**: {total_images} å¼µåœ–ç‰‡\n")
            self.markdown_content.append(f"**åœ–ç‰‡ä¿å­˜ç›®éŒ„**: `{self.images_dir}/`\n")
            
            # å¯«å…¥æ–‡ä»¶
            with open(self.output_filename, 'w', encoding='utf-8') as f:
                f.writelines(self.markdown_content)
            
            self.logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {self.output_filename}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜Markdownæ–‡ä»¶æ™‚å‡ºéŒ¯: {str(e)}")
    
    def closed(self, reason):
        """çˆ¬èŸ²çµæŸæ™‚é—œé–‰ç€è¦½å™¨"""
        self.driver.quit()
        self.session.close()  # é—œé–‰requests session
        self.logger.info("ç€è¦½å™¨å·²é—œé–‰")
        self.logger.info(f"Markdownå ±å‘Šå·²ä¿å­˜è‡³: {self.output_filename}")
        self.logger.info(f"åœ–ç‰‡å·²ä¿å­˜è‡³ç›®éŒ„: {self.images_dir}/")


# é‹è¡Œè…³æœ¬
if __name__ == "__main__":
    # å‰µå»º Scrapy è¨­ç½®
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
    
    # è¨­ç½®è¼¸å‡ºæ ¼å¼ - ä¿ç•™JSONä½œç‚ºå‚™ä»½
    settings = get_project_settings()
    settings.set('FEEDS', {
        'tradingview_news_backup.json': {
            'format': 'json',
            'encoding': 'utf8',
            'store_empty': False,
            'fields': ['index', 'list_title', 'panel_title', 'content', 'images', 'symbols', 'url'],
            'indent': 2
        }
    })
    
    # è¨­ç½®ç”¨æˆ¶ä»£ç†
    settings.set('USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    # è¨­ç½®ä¸‹è¼‰å»¶é²
    settings.set('DOWNLOAD_DELAY', 2)
    settings.set('RANDOMIZE_DOWNLOAD_DELAY', True)
    
    # å‰µå»ºçˆ¬èŸ²é€²ç¨‹
    process = CrawlerProcess(settings)
    process.crawl(TradingViewNewsSpider)
    process.start()
    
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“„ Markdownå ±å‘Šå·²ä¿å­˜")
    print(f"ğŸ–¼ï¸ åœ–ç‰‡å·²ä¿å­˜è‡³ç›®éŒ„")
    print(f"ğŸ’¾ JSONå‚™ä»½æ–‡ä»¶: tradingview_news_backup.json")