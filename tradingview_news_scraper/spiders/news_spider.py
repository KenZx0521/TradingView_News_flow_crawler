# requirements.txt
# scrapy
# selenium
# webdriver-manager

import scrapy
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import os
from datetime import datetime

class TradingViewNewsSpider(scrapy.Spider):
    name = 'tradingview_news'
    start_urls = ['https://www.tradingview.com/news-flow/?symbol=BINANCE:BTCUSDT']
    
    def __init__(self):
        # è¨­ç½®Chromeé¸é …
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼ï¼Œå¦‚æœéœ€è¦çœ‹åˆ°ç€è¦½å™¨é‹è¡Œè«‹è¨»é‡‹æ‰
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # åˆå§‹åŒ–WebDriver
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
        # XPATHè·¯å¾‘é…ç½® - è«‹åœ¨æ­¤è™•å¡«å…¥æ‚¨çš„XPATH
        self.xpaths = {
            'news_list': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div',  # æ–°èåˆ—è¡¨å®¹å™¨çš„XPATH
            'news_items': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div/a',  # æ–°èé …ç›®çš„XPATHï¼ˆå¯é»æ“Šçš„é€£çµï¼‰
            'right_panel': '//*[@id="news-screener-page"]/div/div/div/div[3]',  # å³å´é¢æ¿çš„XPATH
            'panel_title': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/h2',  # å³å´é¢æ¿æ¨™é¡Œçš„XPATH
            'panel_content': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/div[3]/div/div[1]/div[2]/span',  # å³å´é¢æ¿å…§å®¹çš„XPATH
            'panel_symbols': '//*[@id="news-screener-page"]/div/div/div/div[3]/div/div/div/div/article/div[3]/div/div[1]/div[1]',  # å³å´é¢æ¿ç¬¦è™Ÿçš„XPATH
            'close_button': '',  # é—œé–‰æŒ‰éˆ•çš„XPATHï¼ˆå¯é¸ï¼‰
            'list_title': '//*[@id="news-screener-page"]/div/div/div/div[1]/div/div[2]/div[2]/div[2]/div/a[1]/article/div/div'  # åˆ—è¡¨ä¸­æ¨™é¡Œçš„XPATHï¼ˆç›¸å°æ–¼news_itemsï¼‰
        }
        
        # åˆå§‹åŒ–Markdownè¼¸å‡º
        self.markdown_content = []
        self.output_filename = f'tradingview_news_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        
    def parse(self, response):
        """ä¸»è¦è§£æå‡½æ•¸"""
        self.driver.get(response.url)
        
        # åˆå§‹åŒ–Markdownæ–‡ä»¶æ¨™é¡Œ
        self.markdown_content.append(f"# TradingView BTCUSDT æ–°èå ±å‘Š\n")
        self.markdown_content.append(f"**çˆ¬å–æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.markdown_content.append(f"**ä¾†æº**: {response.url}\n\n")
        self.markdown_content.append("---\n\n")
        
        try:
            # ç­‰å¾…æ–°èåˆ—è¡¨åŠ è¼‰
            if self.xpaths['news_list']:
                self.wait.until(EC.presence_of_element_located((By.XPATH, self.xpaths['news_list'])))
                self.logger.info("æ–°èåˆ—è¡¨åŠ è¼‰å®Œæˆ")
            
            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ›´å¤šæ–°è
            self.scroll_and_load_news()
            
            # ç²å–æ‰€æœ‰æ–°èé …ç›®
            if not self.xpaths['news_items']:
                self.logger.error("è«‹è¨­ç½® news_items çš„XPATHè·¯å¾‘")
                return
                
            news_items = self.driver.find_elements(By.XPATH, self.xpaths['news_items'])
            self.logger.info(f"æ‰¾åˆ° {len(news_items)} æ¢æ–°è")
            
            # éæ­·æ¯å€‹æ–°èé …ç›®
            for index, news_item in enumerate(news_items):
                try:
                    # ç²å–æ–°èæ¨™é¡Œ
                    title = self.get_news_title(news_item)
                    
                    # é»æ“Šæ–°èé …ç›®
                    self.driver.execute_script("arguments[0].click();", news_item)
                    
                    # ç­‰å¾…å³å´é¢æ¿åŠ è¼‰
                    time.sleep(2)
                    
                    # æå–æ–°èè©³ç´°å…§å®¹
                    news_data = self.extract_news_details(title, index)
                    
                    if news_data:
                        # æ·»åŠ åˆ°Markdownå…§å®¹
                        self.add_to_markdown(news_data)
                        yield news_data
                    
                    # é—œé–‰å³å´é¢æ¿
                    self.close_right_panel()
                    
                except Exception as e:
                    self.logger.error(f"è™•ç†ç¬¬ {index+1} æ¢æ–°èæ™‚å‡ºéŒ¯: {str(e)}")
                    continue
                    
        except TimeoutException:
            self.logger.error("é é¢åŠ è¼‰è¶…æ™‚")
        except Exception as e:
            self.logger.error(f"çˆ¬å–éç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        self.save_markdown_file()
    
    def scroll_and_load_news(self):
        """æ»¾å‹•é é¢åŠ è¼‰æ›´å¤šæ–°è"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scrolls = 5  # é™åˆ¶æ»¾å‹•æ¬¡æ•¸
        
        while scroll_attempts < max_scrolls:
            # æ»¾å‹•åˆ°é é¢åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # ç­‰å¾…åŠ è¼‰
            
            # è¨ˆç®—æ–°çš„é é¢é«˜åº¦
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
                
            last_height = new_height
            scroll_attempts += 1
            
        self.logger.info(f"å®Œæˆ {scroll_attempts} æ¬¡æ»¾å‹•åŠ è¼‰")
    
    def get_news_title(self, news_item):
        """ç²å–æ–°èæ¨™é¡Œ"""
        try:
            if self.xpaths['list_title']:
                # ä½¿ç”¨ç›¸å°XPATHæŸ¥æ‰¾æ¨™é¡Œ
                title_element = news_item.find_element(By.XPATH, self.xpaths['list_title'])
                title = title_element.text.strip()
                if title:
                    return title
            
            # å¦‚æœæ²’æœ‰è¨­ç½®XPATHæˆ–æ‰¾ä¸åˆ°ï¼Œè¿”å›å…ƒç´ æ–‡æœ¬
            return news_item.text.strip() or "ç„¡æ¨™é¡Œ"
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨™é¡Œæ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ¨™é¡Œ"
    
    def extract_news_details(self, title, index):
        """æå–æ–°èè©³ç´°å…§å®¹"""
        try:
            # ç­‰å¾…å³å´é¢æ¿å‡ºç¾
            if not self.xpaths['right_panel']:
                self.logger.error("è«‹è¨­ç½® right_panel çš„XPATHè·¯å¾‘")
                return None
                
            right_panel = self.wait.until(
                EC.presence_of_element_located((By.XPATH, self.xpaths['right_panel']))
            )
            
            # æå–æ¨™é¡Œï¼ˆå¾å³å´é¢æ¿ï¼‰
            panel_title = self.extract_panel_title()
            
            # æå–å…§å®¹
            content = self.extract_content()
            
            # æå–ç›¸é—œç¬¦è™Ÿ
            symbols = self.extract_symbols()
            
            news_data = {
                'index': index + 1,
                'list_title': title,  # åˆ—è¡¨ä¸­çš„æ¨™é¡Œ
                'panel_title': panel_title,  # å³å´é¢æ¿ä¸­çš„æ¨™é¡Œ
                'content': content,
                'symbols': symbols,
                'url': self.driver.current_url
            }
            
            self.logger.info(f"æˆåŠŸæå–ç¬¬ {index+1} æ¢æ–°è: {panel_title[:50]}...")
            return news_data
            
        except TimeoutException:
            self.logger.error(f"ç¬¬ {index+1} æ¢æ–°èå³å´é¢æ¿åŠ è¼‰è¶…æ™‚")
            return None
        except Exception as e:
            self.logger.error(f"æå–ç¬¬ {index+1} æ¢æ–°èè©³æƒ…æ™‚å‡ºéŒ¯: {str(e)}")
            return None
    
    def extract_panel_title(self):
        """å¾å³å´é¢æ¿æå–æ¨™é¡Œ"""
        try:
            if self.xpaths['panel_title']:
                title_element = self.driver.find_element(By.XPATH, self.xpaths['panel_title'])
                return title_element.text.strip()
            else:
                self.logger.warning("æœªè¨­ç½® panel_title çš„XPATHè·¯å¾‘")
                return "ç„¡æ¨™é¡Œ"
        except NoSuchElementException:
            self.logger.warning("ä½¿ç”¨XPATHæœªæ‰¾åˆ°æ¨™é¡Œå…ƒç´ ")
            return "ç„¡æ¨™é¡Œ"
        except Exception as e:
            self.logger.error(f"æå–é¢æ¿æ¨™é¡Œæ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡æ¨™é¡Œ"
    
    def extract_content(self):
        """æå–æ–°èå…§å®¹"""
        try:
            if self.xpaths['panel_content']:
                # æ”¯æŒå–®å€‹æˆ–å¤šå€‹å…§å®¹å…ƒç´ 
                content_elements = self.driver.find_elements(By.XPATH, self.xpaths['panel_content'])
                
                contents = []
                for element in content_elements:
                    text = element.text.strip()
                    if text and len(text) > 10:  # éæ¿¾æ‰å¤ªçŸ­çš„æ–‡æœ¬
                        contents.append(text)
                
                # å»é‡ä¸¦åˆä½µå…§å®¹
                unique_contents = list(dict.fromkeys(contents))  # ä¿æŒé †åºçš„å»é‡
                return ' '.join(unique_contents) if unique_contents else "ç„¡å…§å®¹"
            else:
                self.logger.warning("æœªè¨­ç½® panel_content çš„XPATHè·¯å¾‘")
                return "ç„¡å…§å®¹"
                
        except Exception as e:
            self.logger.error(f"æå–å…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç„¡å…§å®¹"
    
    def extract_symbols(self):
        """æå–ç›¸é—œç¬¦è™Ÿ"""
        try:
            if self.xpaths['panel_symbols']:
                symbol_elements = self.driver.find_elements(By.XPATH, self.xpaths['panel_symbols'])
                
                symbols = []
                for element in symbol_elements:
                    text = element.text.strip()
                    if text:
                        symbols.append(text)
                
                return list(set(symbols)) if symbols else []
            else:
                self.logger.warning("æœªè¨­ç½® panel_symbols çš„XPATHè·¯å¾‘")
                return []
                
        except Exception as e:
            self.logger.error(f"æå–ç¬¦è™Ÿæ™‚å‡ºéŒ¯: {str(e)}")
            return []
    
    def close_right_panel(self):
        """é—œé–‰å³å´é¢æ¿"""
        try:
            if self.xpaths['close_button']:
                # ä½¿ç”¨è¨­ç½®çš„é—œé–‰æŒ‰éˆ•XPATH
                close_btn = self.driver.find_element(By.XPATH, self.xpaths['close_button'])
                close_btn.click()
                time.sleep(1)
                return
            
            # å¦‚æœæ²’æœ‰è¨­ç½®é—œé–‰æŒ‰éˆ•XPATHï¼Œé»æ“Šé¢æ¿å¤–éƒ¨å€åŸŸ
            self.driver.execute_script("document.body.click();")
            time.sleep(1)
            
        except NoSuchElementException:
            # å¦‚æœæ‰¾ä¸åˆ°é—œé–‰æŒ‰éˆ•ï¼Œå˜—è©¦é»æ“Šé é¢å…¶ä»–å€åŸŸ
            self.driver.execute_script("document.body.click();")
            time.sleep(1)
        except Exception as e:
            self.logger.error(f"é—œé–‰å³å´é¢æ¿æ™‚å‡ºéŒ¯: {str(e)}")
    
    def add_to_markdown(self, news_data):
        """å°‡æ–°èæ•¸æ“šæ·»åŠ åˆ°Markdownå…§å®¹ä¸­"""
        try:
            self.markdown_content.append(f"## {news_data['index']}. {news_data['panel_title']}\n\n")
            
            # å¦‚æœåˆ—è¡¨æ¨™é¡Œèˆ‡é¢æ¿æ¨™é¡Œä¸åŒï¼Œé¡¯ç¤ºåˆ—è¡¨æ¨™é¡Œ
            if news_data['list_title'] != news_data['panel_title'] and news_data['list_title'] != "ç„¡æ¨™é¡Œ":
                self.markdown_content.append(f"**åˆ—è¡¨æ¨™é¡Œ**: {news_data['list_title']}\n\n")
            
            # æ–°èå…§å®¹
            if news_data['content'] and news_data['content'] != "ç„¡å…§å®¹":
                self.markdown_content.append(f"**å…§å®¹**:\n{news_data['content']}\n\n")
            
            # ç›¸é—œç¬¦è™Ÿ
            if news_data['symbols']:
                symbols_text = ", ".join([f"`{symbol}`" for symbol in news_data['symbols']])
                self.markdown_content.append(f"**ç›¸é—œç¬¦è™Ÿ**: {symbols_text}\n\n")
            
            # ä¾†æºé€£çµ
            if news_data['url']:
                self.markdown_content.append(f"**ä¾†æº**: [TradingView]({news_data['url']})\n\n")
            
            self.markdown_content.append("---\n\n")
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ Markdownå…§å®¹æ™‚å‡ºéŒ¯: {str(e)}")
    
    def save_markdown_file(self):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        try:
            # æ·»åŠ æ–‡ä»¶çµå°¾
            self.markdown_content.append(f"\n---\n")
            self.markdown_content.append(f"**å ±å‘Šç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            self.markdown_content.append(f"**ç¸½å…±çˆ¬å–**: {len([line for line in self.markdown_content if line.startswith('## ')])} æ¢æ–°è\n")
            
            # å¯«å…¥æ–‡ä»¶
            with open(self.output_filename, 'w', encoding='utf-8') as f:
                f.writelines(self.markdown_content)
            
            self.logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {self.output_filename}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜Markdownæ–‡ä»¶æ™‚å‡ºéŒ¯: {str(e)}")
    
    def closed(self, reason):
        """çˆ¬èŸ²çµæŸæ™‚é—œé–‰ç€è¦½å™¨"""
        self.driver.quit()
        self.logger.info("ç€è¦½å™¨å·²é—œé–‰")
        self.logger.info(f"Markdownå ±å‘Šå·²ä¿å­˜è‡³: {self.output_filename}")


# é‹è¡Œè…³æœ¬
if __name__ == "__main__":
    # å‰µå»º Scrapy è¨­ç½®
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
    
    # è¨­ç½®è¼¸å‡ºæ ¼å¼ - ä¿ç•™JSONä½œç‚ºå‚™ä»½
    settings = get_project_settings()
    settings.set('FEEDS', {
        'tradingview_news_backup.json': {
            'format': 'json',
            'encoding': 'utf8',
            'store_empty': False,
            'fields': ['index', 'list_title', 'panel_title', 'content', 'symbols', 'url'],
            'indent': 2
        }
    })
    
    # è¨­ç½®ç”¨æˆ¶ä»£ç†
    settings.set('USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    # è¨­ç½®ä¸‹è¼‰å»¶é²
    settings.set('DOWNLOAD_DELAY', 2)
    settings.set('RANDOMIZE_DOWNLOAD_DELAY', True)
    
    # å‰µå»ºçˆ¬èŸ²é€²ç¨‹
    process = CrawlerProcess(settings)
    process.crawl(TradingViewNewsSpider)
    process.start()
    
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“„ Markdownå ±å‘Šå·²ä¿å­˜")
    print(f"ğŸ’¾ JSONå‚™ä»½æ–‡ä»¶: tradingview_news_backup.json")